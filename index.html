<!DOCTYPE html>
<html lang="en">
    <head>
	<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="description" content="">
	<meta name="author" content="">
	<title>Social Intelligence Workshop</title>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

	  <meta charset="utf-8">
	  <meta name="viewport" content="width=device-width, initial-scale=1">
	  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
	  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
	  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

	<!-- Latest compiled and minified JavaScript -->
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


	<link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
	<!-- Custom styles for this template -->


	<link href="../css/scrolling-nav.css" rel="stylesheet">
	<link href="../css/style.css" rel="author stylesheet">
	    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0M445FTS98"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0M445FTS98');
</script>
    </head>

    <body id="page-top">

	<!-- Navigation -->
	<nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
	    <div class="container bar-container">
		<a class="title-head" href="#page-top">Social Intelligence in Humans and Robots</a>
		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
		    <span class="navbar-toggler-icon"></span>
		</button>
		<div class="collapse navbar-collapse" id="navbarResponsive">
		    <ul class="navbar-nav ml-auto">
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#about">About</a>
				</li>


				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
				</li>

				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
				</li>
<!-- 				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#callpapers"> Call for papers </a>
				</li>  -->
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#papers"> Papers </a>
				</li>
				<!-- <li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
				</li> -->
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
				</li>
				<li class="nav-item dropdown">
				    <a class="nav-link dropdown-toggle" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">All Workshops</a>
				        <div class="dropdown-menu">
					      <a class="dropdown-item" href="https://social-intelligence-human-ai.github.io/ICRA2021">ICRA2021</a>
					      <a class="dropdown-item" href="https://social-intelligence-human-ai.github.io/ICLRSocial">ICLR Social</a>
					      <a class="dropdown-item" href="https://social-intelligence-human-ai.github.io/RSS2022">RSS2022</a>
					      <a class="dropdown-item" href="https://social-intelligence-human-ai.github.io/RSS2023">RSS2023</a>
					    </div>
				</li>
		    </ul>
		</div>
	    </div>
	</nav>

	<header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
	    <div style="background-color: rgba(160,160,160,0)" class="text-center">
		<div style="padding-bottom: 6%; padding-top: 6%; background-image: url('../images/banner3.png'); background-size: cover">
	    <div class="container titlebox"; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
		<p style="text-align: center; margin-bottom: 2" class="title">Social Intelligence in Humans and Robots</p>
		<p style="text-align: center; margin-bottom: 0" class="subtitle">Workshop RSS 2024 - July 19 (1:45pm - 6pm in Delft, Netherlands)</p>
		<!-- <p style="text-align: center; margin-bottom: 0" class="subtitle">Our paper submission is hosted on OpenReview. Here is the <a href="https://openreview.net/group?id=roboticsfoundation.org/RSS/2024/Workshop/SIHR">paper submission website</a></p> -->
		<p style="text-align: center; margin-bottom: 0" class="subtitle">In person locatoin: ME Hall D - Watt</p>
		<p style="text-align: center; margin-bottom: 0" class="subtitle">Zoom link: https://wse.zoom.us/j/96974865033</p>
		<!-- <p style="text-align: center; margin-bottom: 0" class="subtitle">In person poster session: CS Lounge (4th floor in Mudd Bldg); Gather.Town session: <a href="https://app.gather.town/events/Pw6hd48Tc4xszTpmpxZO">Link</a></p> -->
		<!-- <p style="text-align: center; margin-bottom: 0" class="subtitle">Video recordings are available on our YouTube channel: <a href=" https://youtube.com/playlist?list=PL1Rlxzv3f7yvg6hVupmIu4MV6lha9vLN4">link</a></p> -->

		</div>
	    </div>
	    </div>in
	</header>


<!-- 	<hr class="half-rule"/>
	    <section style="background: rgba(0,0,0,0.9); padding: 20px 0px">
	    <div class="container">
		<div class="row">
			
<!-- 		    <div class="col-md-10 mx-auto" style="text-align: center;">
			    
			     <span style="font-size: 1.3em; color: white">
				    Post your questions on  <a href="https://app.sli.do/event/nsviw14p/live/questions"> Sli.do </a>
			    </span>
			    <br>
			    
			    <br>
			    <span style="font-size: 1.3em; color: white">
				    Follow <a href="https://twitter.com/SIHRworkshop">@SIHRworkshop</a> for updates
			    </span>
			</div>
		    </div> -->
<!-- 	    </div> -->
<!-- 	    </section> --> 
	    <hr class="half-rule"/>
	<section id="about">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>About</span>
			<br>
			<span>
Social intelligence is at the core of both human and artificial intelligence. From a young age, humans can understand, interact, collaborate, and communicate with each other. Most of what we learn is taught by others, or learned in a social context. Thus, a truly intelligent AI agent should be able to understand and work with humans as well as other AI agents. 
<br><br>
This workshop focuses on the challenges and developments in building AI systems equipped with social intelligence, and leverages theories and insights from studies of human social intelligence for achieving such goals. In particular, the workshop will explore what it would take for machines to:
<br><br>
<ol>
	<li style="display: list-item">Understand the behaviors and mental states of humans </li>
	<li style="display: list-item">Engage in rich and complex interactions with humans</li>
	
</ol>
The workshop will bring together experts from cognitive science and developmental psychology to better understand the principles and origins of human social intelligence, and experts from AI and Robotics, to discuss how to engineer socially intelligent artificial agents, and how these paradigms can be deployed in both virtual and real scenarios. 

<!-- The workshop will adopt a hybrid format, including in-person presenations, live streams, and a hybrid poster session. -->
			    

			    </span>
		    </div>
		    <div class="col-md-7 mx-auto" style="text-align: center;">
			<br>


		    </div>
		</div>

	    </div>



	</section>


<hr class="half-rule"/>
	<section id="speakers">

	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">

		<span class=titlesec>Speakers and Panelists</span><br>

		<br><br>

		<div class="row">
			
			<a href='https://www.mit.edu/~jda/'>
		    <div class="profpic xlarge-1 columns">

			<img  src=../images/people/jacob_andreas.jpg class="figure-img img-fluid ">
			<p class=profname>  <a href="https://www.mit.edu/~jda/"> Jacob Andreas </a> </p>
			<p class=institution>Massachusetts Institute of Technology</p>
			<p class=institution>Confirmed (Remote) </p>
		    </div>
			</a>
			
			<a href='https://andreea7b.github.io'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/abobu2.jpg class="figure-img img-fluid ">
			<p class=profname><a href="https://andreea7b.github.io">Andreea Bobu</a></p>
			<p class=institution>Boston Dynamics AI Institute / MIT</p>
			<p class=institution>Confirmed (Remote) </p>
		    </div>
			</a>

			<a href='https://samueli.ucla.edu/people/veronica-santos/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src=../images/people/YonatanBisk_400.jpg class="figure-img img-fluid ">
			<p class=profname><a href="https://talkingtorobots.com/yonatanbisk.html">Yonatan Bisk</a></p>
			<p class=institution> Carnegie Mellon University</p>
			<p class=institution> Confirmed (In-Person)</p>
		    </div>
			</a>


		</div>
		<div class="row">
			<a href='https://michael-franke.github.io/heimseite/'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/wald.png class="figure-img img-fluid ">
			<p class=profname><a href="https://michael-franke.github.io/heimseite/"> Michael Franke </a></p>
			<p class=institution> University of Tübingen</p>
			<p class=institution> Confirmed (In-Person) </p>
		    </div>
			</a>
			<a href='https://academia.skadge.org'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/Lemaignan.jpg class="figure-img img-fluid ">
			<p class="profname"> <a href="https://academia.skadge.org"> Séverin Lemaignan </a></p>
			<p class=institution>PAL Robotics </p>
			<p class=institution> Confirmed (Remote) </p>
		    </div>
			</a>

			<a href='https://research.google/people/carolina-parada/'>
				<div class="profpic xlarge-1 columns">
					<img  src=../images/people/carolina_parada.png class="figure-img img-fluid ">
				<p class=profname><a href="https://research.google/people/carolina-parada/">Carolina Parada</a></p>
				<p class=institution> Google Deepmind</p>
				<p class=institution> Confirmed (In-Person)</p>
				</div>
			</a>

		   
		</div>

		<!-- <div class="row"> -->
	
		 

			<!-- <a href='http://www.leilatakayama.org/wp/about-leila/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src=../images/people/leila.jpeg class="figure-img img-fluid ">
			<p class=profname><a href="http://www.leilatakayama.org/wp/about-leila/">Leila Takayama</a></p>
			<p class=institution> Hoku Labs</p>
			<p class=institution> Confirmed (Remote) </p>
		    </div>
		   </a> -->
		<!-- </div> -->


		</div>
</div>

	</section>


	<!-- <hr class="half-rule"/>
	

	<section id="speakers">
	    <div class="container">
			<span>
				
			</span>
			</span>
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec> Speakers and Panelists</span><br>
		<div class="row">
			<a href="https://markkho.github.io/">
				<div class="profpic speaker xlarge-1 columns">
					 <img  src="../images/people_rss_23/markkho.jpg" class="figure-img img-fluid ">
				 <p class=profname> <a href="https://markkho.github.io/"> Mark Ho </a></p>
				 <p class=institution>  New York University </p>
				 <p class=institution><br> Confirmed (Remote) </p>
	 
				 </div>
				 </a>

			<a href='https://www.ri.cmu.edu/ri-faculty/henny-admoni/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img src="../images/people_rss_23/profile_admoni.jpg" class="figure-img img-fluid ">
			<p class=profname><a href="https://www.ri.cmu.edu/ri-faculty/henny-admoni/"> Henny Admoni </a> </p>
			<p class=institution> Carnegie Mellon University </p>
			<p class=institution><br> Confirmed (Remote) </p>
		    </div>
			</a>

			<a href='https://developmental-robotics.jp/en/members/yukie_nagai/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src="../images/people_rss_23/profile_yukie.png" class="figure-img img-fluid ">
			<p class=profname><a href="https://developmental-robotics.jp/en/members/yukie_nagai/">  Yukie Nagai
			</a></p>
			<p class=institution> The University of Tokyo </p>
			<p class=institution><br> Confirmed </p>
		    </div>

		</div>

		<div class="row">

		</a>
		<a href="https://people.cs.umass.edu/~sniekum/">
		<div class="profpic speaker xlarge-1 columns">
			<img  src='../images/people_rss_23/profile_niekum.jpg' class="figure-img img-fluid ">
		<p class=profname><a href="https://people.cs.umass.edu/~sniekum/"> Scott Niekum
		</a></p>
		<p class=institution>UMass Amherst</p>
		<p class=institution><br> Confirmed (Remote) </p>

		</div>
		</a>

			<a href='https://pnc.unipd.it/farroni-teresa/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src='https://pnc.unipd.it/wp-content/uploads/2017/10/farroni_pnc.jpg' class="figure-img img-fluid ">
			<p class="profname"> <a href="https://pnc.unipd.it/farroni-teresa/"> Teresa Farroni </a></p>
			<p class=institution><br> University of Padua </p>
			<p class=institution><br>Awaiting Confirmation</p>
		    </div>
			</a>

			<a href='https://www.yangwulab.com/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src='../images/people_rss_23/profile_yangwu.png' class="figure-img img-fluid ">
			<p class="profname"> <a href="https://www.yangwulab.com/">  Yang Wu  </a></p>
			<p class=institution>University of Toronto</p>
			<p class=institution><br> Confirmed (Remote) </p>
		    </div>
			</a>
			


		</div>


		</div>
</div>

	</section> -->

	<!-- <hr class="half-rule"/>
	<section class="">
	    <div class="container" id="schedule">
		<div class="row">
			<div class="col-md-10 mx-auto">
			<span class="titlesec"><span></span>Schedule</span>
			<br><br>
			Schedule TBA 
		    

		    </div>
		</div>
	    </div>
	</section> -->

	<hr class="half-rule"/>
	<section class="">
	    <div class="container" id="schedule">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class="titlesec"><span></span>Schedule</span>
			<br><br>
			<table class="table table-striped">
				<tbody>
				<tr>
					<th> Time (Delft Time, Netherlands)</th> 
				</tr>
				<tr>
					<td>	1:45 pm - 2:00 pm        </td><td>  Organizers <br> <b> Introductory Remarks  </b> </td>
				</tr>

				<tr>
					<td>	
						2:00 pm - 2:30 pm      
					</td>
					<td>   Carolina Parada <br> 
						<b> Foundation Models for Social Robots </b> 
						<br>
						<a data-toggle="collapse" data-target="#abstractjuliandf" class="collapsed abstract" aria-expanded="false"> Abstract</a>
						<div id="abstractjuliandf" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									Foundation models have unlocked major advancements in AI. In this talk, I will discuss examples of how foundation models are enabling a step function in progress towards more social robots, including enabling robots to understand, reason in context, hold situated conversations with humans, create expressive robot behaviors, transfer visual and semantic understanding to real world actions, and learn from humans as they interact with them. It is still early in this research journey but it is an exciting one because we can confidently be part of this fantastic fast and dynamic field of foundation models and not only ride the wave of innovation, but help shape it. Foundation models still have significant gaps in human-robot interaction contexts. I will share early insights showing that HRI could be key to evolving the foundation models themselves, enabling even more powerful interactions, and improving robot learning over time.
						</div>
					</td>
				</tr>

				<tr>
					<td>	
						2:30 pm - 3:00 pm     
					</td>
					<td>   Yonatan Bisk <br> 
						<b> Everything Fails, Everything is Ambiguous </b> 
						<br>
						<a data-toggle="collapse" data-target="#abstractjuliandf" class="collapsed abstract" aria-expanded="false"> Abstract</a>
						<div id="abstractjuliandf" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									 It's easiest to assume we live in a world of perfectly executable programs, where the context is clear, modules work, and language is unambiguous.  In fact, many of us pretend that is the case, eschewing issues of uncertainty or theory of mind.  But in the real world, tools (real and LM) fail, language is ambiguous, and the people around us change the context.  So, how do we build agents that account for all of these variables, and what new challenges do they introduce? In this talk, I'll introduce how we've been thinking about the problem, and some preliminary steps.
						</div>
					</td>
				</tr>

				<tr>
					<td>	
						3:00 pm - 3:30 pm   
					</td>
					<td>   Andreea Bobu <br> 
						<b> Aligning Robot and Human Representations </b> 
						<br>
						<a data-toggle="collapse" data-target="#abstractjuliandf" class="collapsed abstract" aria-expanded="false"> Abstract</a>
						<div id="abstractjuliandf" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									To perform tasks that humans want in the world, robots rely on a representation of salient task features; for example, to hand me a cup of coffee, the robot considers features like efficiency and cup orientation in its behavior. Prior methods try to learn both a representation and a downstream task jointly from data sets of human behavior, but this unfortunately picks up on spurious correlations and results in behaviors that do not generalize. In my view, what’s holding us back from successful human-robot interaction is that human and robot representations are often misaligned: for example, our assistive robot moved a cup inches away from my face -- which is technically collision-free behavior -- because it lacked an understanding of personal space. Instead of treating people as static data sources, my key insight is that robots must engage with humans in an interactive process for finding a shared representation for more efficient, transparent, and seamless downstream learning. In this talk, I focus on a divide and conquer approach: explicitly focus human input on teaching robots good representations before using them for learning downstream tasks. This means that instead of relying on inputs designed to teach the representation implicitly, we have the opportunity to design human input that is explicitly targeted at teaching the representation and can do so efficiently. I introduce a new type of representation-specific input that lets the human teach new features, I enable robots to reason about the uncertainty in their current representation and automatically detect misalignment, and I propose a novel human behavior model to learn robust behaviors on top of human-aligned representations. By explicitly tackling representation alignment, I believe we can ultimately achieve seamless interaction with humans where each agent truly grasps why the other behaves the way they do.
						</div>
					</td>
				</tr>


				<tr>
					<td>	
						3:30 pm - 4:00 pm 
					</td>
					<td>   Break and Poster Session <br> 
						<!-- <b> Stigma Against AI Companions </b>  -->
						<!-- <br>
						<a data-toggle="collapse" data-target="#abstractjuliandf" class="collapsed abstract" aria-expanded="false"> Abstract</a>
						<div id="abstractjuliandf" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									Amid a 'loneliness pandemic', there has been a rise in ‘companion chatbot’ applications, designed for free-form social conversation that is non-judgmental and available 24/7. Yet we find a robust barrier to their adoption: stigma against friendships and relationship with AI, rooted in the intuition that these relationships are one-sided because AI companions cannot truly understand you. We explore interventions to overcome this barrier. 
						</div> -->
					</td>
				</tr>

				<tr>
					<td>	
						4:00 pm - 4:30 pm
					</td>
					<td>   Contributed Talks <br> 
						<ul>
							<li>
								A Framework for Music-Evoked Autobiographical Memories in Robot-Assisted Tasks for People with Dementia
							</li>
							<li>
								AToM-Bot: Embodied Fulfillment of Unspoken Human Needs with Affective Theory of Mind
							</li>
							<li>
								Enhancing Older Adults' Prospective Memory and Experience with Personalized Reminders: Design of the MemFlow Robot Framework
							</li>
							<li>
								Find It Like a Dog: Using Gesture to Improve Robot Object Search
							</li>
						</ul>
					</td>
				</tr>
				
				<tr>
					<td>	
						4:30 pm - 5:00 pm
					</td>
					<td>   Michael Franke <br> 
						<b> Understanding Language Models: On Japanese Rooms & Minimal World Models </b> 
						<br>
						<a data-toggle="collapse" data-target="#abstractjuliandf" class="collapsed abstract" aria-expanded="false"> Abstract</a>
						<div id="abstractjuliandf" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									Searle’s famous Chinese Room Argument is an excellent tool for probing our intuitions about why rule-based AI systems are not felt to develop internal understanding in spite of superficially great input-output performance in language use. Building on previous related work (e.g., Bender & Koller’s Octopus Test), I present a thought experiment more parallel to Searle’s, which I call the Japanese Room Argument, to serve as a scaffolding for intuitions about whether language models generate a form of language understanding /by necessity/ if scaled in training size and model capacity to approximate perfect input-output alignment with humans. To complement the intuitive JRA, I also present a formal Minimal Models Argument, which goes roughly like this: if world models for humans and LMs are close to optimal for their respective purposes, the world models of LMs will almost surely be different, as human representations are likely to be co-optimized for multiple tasks, including non-linguistic tasks and linguistic tasks that require /normative/ social interaction embedded in time.
						</div>
					</td>
				</tr>

				<tr>
					<td>	
						5:00 pm - 5:30 pm
					</td>
					<td>   Séverin Lemaignan <br> 
						<b> Modelling the social sphere in the age of LLMs</b> 
						<br>
						<a data-toggle="collapse" data-target="#abstractjuliandf" class="collapsed abstract" aria-expanded="false"> Abstract</a>
						<div id="abstractjuliandf" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									While LLMs and related foundational models are transforming how we interact with robots, most of the research to date focuses on mobile manipulation tasks, usually not accounting much for social interactions. One key to unlock this 'social intelligence for robots' is the design of a general model of the social sphere -- relationships between people, their mental state, their semantic-rich interactions with their environment -- that would be appropriate for integration with eg LLMs. In this talk, I will present our efforts in this direction, introducing our preliminary work on *social embeddings*, and how they can be operationalized on real-world cognitive architectures for interactive robots.

						</div>
					</td>
				</tr>

				<tr>
					<td>	
						5:30 pm - 6:00 pm
					</td>
					<td>   Jacob Andreas <br> 
						<b> Good Old-fashioned LLMs (or, Autoformalizing the World)</b> 
						<br>
						<a data-toggle="collapse" data-target="#abstractjuliandf" class="collapsed abstract" aria-expanded="false"> Abstract</a>
						<div id="abstractjuliandf" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									Classical formal approaches to artificial intelligence, based on manipulation of symbolic structures, have a number of appealing properties---they generalize (and fail) in predictable ways, provide interpretable traces of behavior, and can be formally verified or manually audited for correctness. Why are they so rarely used in the modern era? One of the major challenges in the development of symbolic AI systems is what McCarthy called the "frame problem": the impossibility of enumerating a set of symbolic rules that fully characterize the behavior of every system in every circumstance. Modern deep learning approaches avoid this representational challenge, but at the cost of interpretability, robustness, and sample-efficiency. How do we build learning systems that are as flexible as neural models but as understandable and generalizable as symbolic ones? In this talk, I'll describe a recent line of work aimed at automatically building "just-in-time" formal models tailored to be just expressive enough to solve tasks of interest. In this approach, neural sequence models pre-trained on text and code are used to place priors over symbolic model descriptions, which are then verified and refined interactively, yielding symbolic planning representations for sequential decision-making. The resulting systems provide human-interpretable traces of behavior and can leverage human-like common-sense and background knowledge during planning, without requiring human system designers in the loop.
						</div>
					</td>
				</tr>

				<tr>
					<td>	6:00 pm - 6:02 pm        </td><td>  Organizers <br> <b> Closing Remarks  </b> </td>
				</tr>

				</tbody>

			</table>
			<!-- We have collected a knowledge base of activities people do at home. For every activity, we have descriptions of different ways to perform it, and a program that describes how to execute it. The knowledge base also contains information about states of objects in the environment, and typical locations of those objects.
			<br><br>
			Check out <a href="tools/explore.html">RobotHow</a> for an overview of the knowledge base of activity programs. -->
		    </div>
		</div>
	    </div>
	</section>

	

	<br>
	<hr class="half-rule"/>
	<section id="papers">

		
		
		<div class="container">
			<div class="row">
			    <div class="col-md-10 mx-auto">
					<span class=titlesec>Papers</span><br>
					<!-- Thank you everyone for submitting your papers to this workshop! All the papers below will be presented in the Poster Session (link TBA). Spotlight papers will additionally be presented as short talks during the Spotlight sessions. -->

					<!-- <br><br> -->
					<!-- <h5 style="font-weight: bold"> Spotlights </h5> -->
					<!-- Congratulations to the Spotlight Papers!  -->
					<ul class="listpapers">
	
						<li> 
							 <span class="postername">Paper #1 - <span>
							A Framework for Music-Evoked Autobiographical Memories in Robot-Assisted Tasks for People with Dementia 
								 <!-- <a class="linkpaper" href="./docs/1.pdf"> [link]</a> <br>  -->
							<span class="authorname">
							Paul Raingeard de la Bletiere, Mark Neerincx, Rebecca Schaefer, Catharine Oertel
							</span>
						</li>

						<li> 
							 <span class="postername">Paper #2 - <span>	
							AToM-Bot: Embodied Fulfillment of Unspoken Human Needs with Affective Theory of Mind
								 <!-- <a class="linkpaper" href=""> [link]</a> <br>  -->
							<span class="authorname">
							Fanhong Li, Wei Ding, Ziteng Ji, Zhengrong Xue, Jia Liu
							</span>
						</li>

						<li> 
							 <span class="postername">Paper #3 - <span>	
								
							  Enhancing Older Adults' Prospective Memory and Experience with Personalized Reminders: Design of the MemFlow Robot Framework
								 <!-- <a class="linkpaper" href="./docs/3.pdf"> [link]</a> <br>  -->
							<span class="authorname">
							Yanzhe Li, Bernd Dudzik, Frank Broz, Mark Neerincx
							</span>
						</li>


						<li> 
							 <span class="postername">Paper #4 - <span>	
							Find It Like a Dog: Using Gesture to Improve Robot Object Search
								 <!-- <a class="linkpaper" href="./docs/4.pdf"> [link]</a> <br>  -->
							<span class="authorname">
							Ivy Xiao He, Madeline H. Pelgrim, Kyle Lee, Falak Pabari, Stefanie Tellex, Thao Nguyen, Daphna Buchsbaum
							</span>
						</li>

					</ul>


					<br><br>



					<h5 style="font-weight: bold"> Reviewers </h5>
					We thank the following people for their assistance in reviewing submitted papers.
					<br><br>
					<div class="row">
						<div class="col-md-4">
						<ul>
							
							<li>Abrar Anwar</li>
							<li>Aidan Curtis </li>
							<li>Amar Halilovic </li>
							
							
						</ul>
						</div>
						<div class="col-md-4">
						<ul>

							
							<li>Amy O'Connell </li>
							<li>Leticia Leonor Pinto Alva </li>
							<li>Livia Tomova </li>
							
						</ul>
						</div>
						<div class="col-md-4">
						<ul>
							
							<li>Maria R. Lima </li>
							<li>Marlene Berke</li>
							<li>Nathaniel S. Dennler </li>
							
						</ul>
						</div>
					</div>

				</div>
			</div>
		</div>
	</section>




	<!-- <hr class="half-rule"/>
<section id="callpapers">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Call for papers</span><br>
			   
			    
						    <span style="color:red">New: </span> The call for papers is now open. 
			<br><br>
			<h5 style="font-weight: bold"> Areas of interest </h5>
			We will accept submissions focusing on Human and Robot Social Intelligence. Topics include but are not limited to:
			<br><br>
			<ol>
				<li style="padding-bottom: 10px"> <b> Social Perception and Theory of Mind:</b> How do humans acquire Theory of Mind capabilities? What computational models approximate these capabilities in humans?
				 </li>

				<li style="padding-bottom: 10px"> <b> Human Cooperation Competition and Communication</b>: Origins and development of human communication or competition; Inference of these relationships.
			  </li>
				<li style="padding-bottom: 10px"> <b> Machine Theory of Mind:</b> How to infer what an agent or robot knows about an environment? How to make robots learn about the minds of other robots or humans?
			 </li>
				<li style="padding-bottom: 10px"> <b> Human-Robot Interaction: </b> Methods to understand human intentions or offer assistance; social perception, communication between humans and robots; learning to convey information for better collaboration; probing and gathering information for collaboration. </li>
				<li style="padding-bottom: 10px"> <b> Interdisciplinary Studies: </b> We welcome works that integrate efforts in cognitive sciences and robotics to advance studies in both fields.
				</li>
			</ol>
					We welcome research papers of no more than 4 pages, not including references.
			<br><br>
						    The paper submission deadline is on <b>June 10th (23:59pm, AoE)</b>. Papers should be submitted to <a href="https://openreview.net/group?id=roboticsfoundation.org/RSS/2024/Workshop/SIHR">https://openreview.net/group?id=roboticsfoundation.org/RSS/2024/Workshop/SIHR</a>. Submissions should follow the <a href="https://roboticsconference.org/information/authorinfo/">RSS template</a>, and be submitted as .pdf files. The review process will be double blind, and therefore the papers should be appropriately anonymized.
			<br><br>
						
<!-- All accepted papers will be given oral presentations (lightning talks or spotlight talks) as well as poster presentations. For the oral presentations, authors would have the option to present in-person or remotely. The poster session will be held in-person and virtually on Gather.Town. Accepted papers will be made available online on the workshop website as non-archival reports, allowing submissions to future conferences or journals.    
						    <br><br> -->
		<!-- 	<h5 style="font-weight: bold"> Important Dates </h5>
			<ul>
		

			<li style="display: list-item">
				<b>Submission deadline:</b> Friday, June 10th, 2024 (23:59pm, AoE).
			</li>
			<li style="display: list-item">
				<b>Author Notifications:</b> Friday, July 1st, 2024.
				 
			</li>
				<li style="display: list-item">
					<b>Camera Ready:</b> Friday, July 12th, 2024 (23:59pm, AoE).
				 
			</li>
					<li style="display: list-item">
					<b>Workshop:</b> July 19th, 2024 (2:00 pm - 6:00 pm, Delft Time).
				 
			</li>
			</ul>
	    </div>
		</div>
			</div>
		    </div>
		</div>
	</section>  --> 



	<!-- <hr class="half-rule"/>
	<section id="faq">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>Frequently Asked Questions</span><br>
			For more questions about the workshop, contact us at <a href="mailto:socialintelligenceworkshop@gmail.com">socialintelligenceworkshop@gmail.com</a>.
			<br>
			<br>
					<h5 style="font-weight: bold"> Will the workshop be online?</h5>
					<span> All the workshop be online. Invited talks and spotlights will be recorded, with live Q&A sessions and discussions.</span>
					<br>
					<br>
					<h5 style="font-weight: bold"> How can I attend this workshop?</h5>
					<span> We will be publishing here the Zoom and Gather.Town links to attend the workshop. Stay tuned!   </span>
					<br>
					<br>
					<h5 style="font-weight: bold"> Can I submit previously published work?</h5>
					<span> We welcome new unpublished work into the workshop. You can however submit work that is concurrently under review in a conference or joural. </span>
					<br>
					<br>
					<h5 style="font-weight: bold"> What happens if my paper is accepted at the workshop? </h5>
					<span> We will post your paper in the workshop website, and you will be abe to present a poster through <a href="https://gather.town/">Gather.town</a>. Some selected papers will also have a spotlight presentation, which will be recorded and played during the workshop. Note that accepted papers will be non-archival and they can still be submitted in other journals and conferences.</span>

		    </div>
		</div>
	    </div>
	</section> -->

	<hr class="half-rule"/>
	<section id="organizers">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">

		<span class=titlesec>Organizers</span><br>
		<div class="row">
			
			<a href='https://www.tshu.io/'>
		    <div class="profpic xlarge-1 columns">

			<img  src=../images/people/tshu.png class="figure-img img-fluid ">
			<p class=profname>  <a href="https://www.tshu.io/"> Tianmin Shu </a> </p>
			<p class=institution>Johns Hopkins University</p>
		    </div>
			</a>
			
			<a href='http://people.csail.mit.edu/xavierpuig/'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/xavi.jpg class="figure-img img-fluid ">
			<p class=profname><a href="http://people.csail.mit.edu/xavierpuig">Xavier Puig</a></p>
			<p class=institution>Meta AI</p>
		    </div>
			</a>


			<a href='https://people.csail.mit.edu/lishuang/'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/lishuang.jpg class="figure-img img-fluid ">
			<p class=profname><a href="https://people.csail.mit.edu/lishuang/"> Shuang Li </a> </p>
			<p class=institution> Massachusetts Institute of Technology </p>
		    </div>
			</a>


		</div>
		<div class="row">
			<a href='https://scholar.google.com/citations?user=GA6kDcMAAAAJ&hl=en'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/zhonghaoshi.jpg class="figure-img img-fluid ">
			<p class=profname><a href="https://scholar.google.com/citations?user=GA6kDcMAAAAJ&hl=en"> Zhonghao Shi </a></p>
			<p class=institution> Univeristy of Southern California</p>
		    </div>
			</a>
			<a href='https://www.lintsungchi.com'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/TsungChiLin.png class="figure-img img-fluid ">
			<p class="profname"> <a href="https://www.lintsungchi.com"> Tsung-Chi Lin </a></p>
			<p class=institution>Johns Hopkins University</p>
		    </div>
			</a>

		   <a href='https://andipeng.com'>
		    <div class="profpic xlarge-1 columns">
			    <img  src=../images/people/andipeng.jpg class="figure-img img-fluid ">
			<p class=profname><a href="https://andipeng.com">Andi Peng</a></p>
			<p class=institution> Massachusetts Institute of Technology</p>
		    </div>
		   </a>
		</div>

		<div class="row">
		   <a href='https://avivne.github.io/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src=../images/people/aviv.png class="figure-img img-fluid ">
			<p class=profname><a href="https://avivne.github.io/">Aviv Netanyahu</a></p>
			<p class=institution> Massachusetts Institute of Technology</p>
		    </div>
			</a>

		   <a href='http://web.mit.edu/torralba/www/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src=../images/people/torralba.jpg class="figure-img img-fluid ">
			<p class=profname><a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a></p>
			<p class=institution> Massachusetts Institute of Technology</p>
		    </div>
			</a>
		</div>


		</div>
</div>

	</section>



	<br>
	<hr class="half-rule"/>
	<section id="contact">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Contact</span><br>
		Reach out to Tianmin Shu (<a href="tianmin.shu@jhu.edu">tianmin.shu@jhu.edu</a>) for any questions.
	    </div>
		</div>
		</div>
	</section>

	<!-- Footer -->


	<!-- Bootstrap core JavaScript -->
<!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	<!-- Plugin JavaScript -->
	<!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> --> -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js



"> </script>

	<!-- Custom JavaScript for this theme -->
	<script src="js/scrolling-nav.js"></script>

    </body>


</html>
