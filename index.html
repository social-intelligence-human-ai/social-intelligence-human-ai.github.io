<!DOCTYPE html>
<html lang="en">
    <head>
	<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="description" content="">
	<meta name="author" content="">
	<title>Social Intelligence Workshop</title>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

	  <meta charset="utf-8">
	  <meta name="viewport" content="width=device-width, initial-scale=1">
	  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
	  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
	  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

	<!-- Latest compiled and minified JavaScript -->
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


	<link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
	<!-- Custom styles for this template -->


	<link href="../css/scrolling-nav.css" rel="stylesheet">
	<link href="../css/style.css" rel="author stylesheet">
	    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0M445FTS98"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0M445FTS98');
</script>
    </head>

    <body id="page-top">

	<!-- Navigation -->
	<nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
	    <div class="container bar-container">
		<a class="title-head" href="#page-top">Social Intelligence in Humans and Robots</a>
		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
		    <span class="navbar-toggler-icon"></span>
		</button>
		<div class="collapse navbar-collapse" id="navbarResponsive">
		    <ul class="navbar-nav ml-auto">
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#about">About</a>
				</li>


				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
				</li>

				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
				</li>
<!-- 				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#callpapers"> Call for papers </a>
				</li>  -->
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#papers"> Papers </a>
				</li>
				<!-- <li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
				</li> -->
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
				</li>
				<li class="nav-item dropdown">
				    <a class="nav-link dropdown-toggle" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">All Workshops</a>
				        <div class="dropdown-menu">
					      <a class="dropdown-item" href="https://social-intelligence-human-ai.github.io/ICRA2021">ICRA2021</a>
					      <a class="dropdown-item" href="https://social-intelligence-human-ai.github.io/ICLRSocial">ICLR Social</a>
					      <a class="dropdown-item" href="https://social-intelligence-human-ai.github.io/RSS2022">RSS2022</a>
					    </div>
				</li>
		    </ul>
		</div>
	    </div>
	</nav>

	<header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
	    <div style="background-color: rgba(160,160,160,0)" class="text-center">
		<div style="padding-bottom: 6%; padding-top: 6%; background-image: url('../images/banner3.png'); background-size: cover">
	    <div class="container titlebox"; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
		<p style="text-align: center; margin-bottom: 2" class="title">Social Intelligence in Humans and Robots</p>
		<p style="text-align: center; margin-bottom: 0" class="subtitle">Workshop RSS 2023 - July 9, US Eastern Time / July 10, Korean time</p>
		<p style="text-align: center; margin-bottom: 0" class="subtitle">
		Hybrid, Zoom link will be posted here before the workshop starts</p>
		<p style="text-align: center; margin-bottom: 0" class="subtitle">In-person location: 322B</p>
		<!-- <p style="text-align: center; margin-bottom: 0" class="subtitle">In person locatoin: 545 Mudd</p> -->
		<!-- <p style="text-align: center; margin-bottom: 0" class="subtitle">In person poster session: CS Lounge (4th floor in Mudd Bldg); Gather.Town session: <a href="https://app.gather.town/events/Pw6hd48Tc4xszTpmpxZO">Link</a></p> -->
		<!-- <p style="text-align: center; margin-bottom: 0" class="subtitle">Video recordings are available on our YouTube channel: <a href=" https://youtube.com/playlist?list=PL1Rlxzv3f7yvg6hVupmIu4MV6lha9vLN4">link</a></p> -->

		</div>
	    </div>
	    </div>
	</header>


<!-- 	<hr class="half-rule"/>
	    <section style="background: rgba(0,0,0,0.9); padding: 20px 0px">
	    <div class="container">
		<div class="row">
			
<!-- 		    <div class="col-md-10 mx-auto" style="text-align: center;">
			    
			     <span style="font-size: 1.3em; color: white">
				    Post your questions on  <a href="https://app.sli.do/event/nsviw14p/live/questions"> Sli.do </a>
			    </span>
			    <br>
			    
			    <br>
			    <span style="font-size: 1.3em; color: white">
				    Follow <a href="https://twitter.com/SIHRworkshop">@SIHRworkshop</a> for updates
			    </span>
			</div>
		    </div> -->
<!-- 	    </div> -->
<!-- 	    </section> --> 
	    <hr class="half-rule"/>
	<section id="about">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>About</span>
			<br>
			<span>
Social intelligence is at the core of both human and artificial intelligence. From a young age, humans can understand, interact, collaborate, and communicate with each other. Most of what we learn is taught by others, or learned in a social context. Thus, a truly intelligent AI agent should be able to understand and work with humans as well as other AI agents. 
<br><br>
This workshop focuses on the challenges and developments in building AI systems equipped with social intelligence, and leverages theories and insights from studies of human social intelligence for achieving such goals. In particular, the workshop will explore what it would take for machines to:
<br><br>
<ol>
	<li style="display: list-item">Understand the behaviors and mental states of humans </li>
	<li style="display: list-item">Engage in rich and complex interactions with humans</li>
	
</ol>
The workshop will bring together experts from cognitive science and developmental psychology to better understand the principles and origins of human social intelligence, and experts from AI and Robotics, to discuss how to engineer socially intelligent artificial agents, and how these paradigms can be deployed in both virtual and real scenarios. 

<!-- The workshop will adopt a hybrid format, including in-person presenations, live streams, and a hybrid poster session. -->
			    

			    </span>
		    </div>
		    <div class="col-md-7 mx-auto" style="text-align: center;">
			<br>


		    </div>
		</div>

	    </div>



	</section>





	<hr class="half-rule"/>
	

	<section id="speakers">
	    <div class="container">
			<span>
				
			</span>
			</span>
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec> Speakers</span><br>
		<div class="row">
			<a href="https://markkho.github.io/">
				<div class="profpic speaker xlarge-1 columns">
					 <img  src="../images/people_rss_23/markkho.jpg" class="figure-img img-fluid ">
				 <p class=profname> <a href="https://markkho.github.io/"> Mark Ho </a></p>
				 <p class=institution>  New York University </p>
	 
				 </div>
				 </a>

			<a href='https://www.ri.cmu.edu/ri-faculty/henny-admoni/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img src="../images/people_rss_23/profile_admoni.jpg" class="figure-img img-fluid ">
			<p class=profname><a href="https://www.ri.cmu.edu/ri-faculty/henny-admoni/"> Henny Admoni </a> </p>
			<p class=institution> Carnegie Mellon University </p>
		    </div>
			</a>

			<a href='https://developmental-robotics.jp/en/members/yukie_nagai/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src="../images/people_rss_23/profile_yukie.png" class="figure-img img-fluid ">
			<p class=profname><a href="https://developmental-robotics.jp/en/members/yukie_nagai/">  Yukie Nagai
			</a></p>
			<p class=institution> wUniversity of Tokyo </p>
		    </div>

		</div>
		<div class="row">

		</a>
		<a href="https://people.cs.umass.edu/~sniekum/">
		<div class="profpic speaker xlarge-1 columns">
			<img  src='../images/people_rss_23/profile_niekum.jpg' class="figure-img img-fluid ">
		<p class=profname><a href="https://people.cs.umass.edu/~sniekum/"> Scott Niekum
		</a></p>
		<p class=institution>UMass Amherst</p>

		</div>
		</a>

	<!-- 		<a href='https://pnc.unipd.it/farroni-teresa/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src='https://pnc.unipd.it/wp-content/uploads/2017/10/farroni_pnc.jpg' class="figure-img img-fluid ">
			<p class="profname"> <a href="https://pnc.unipd.it/farroni-teresa/"> Teresa Farroni </a></p>
			<p class=institution><br> University of Padua </p>
			<p class=institution><br>Awaiting Confirmation</p>
		    </div>
			</a> -->

			<a href='https://www.yangwulab.com/'>
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src='../images/people_rss_23/profile_yangwu.png' class="figure-img img-fluid ">
			<p class="profname"> <a href="https://www.yangwulab.com/">  Yang Wu  </a></p>
			<p class=institution>University of Toronto Scarborough</p>
		    </div>
			</a>
			


		</div>


		</div>
</div>

	</section>

	<hr class="half-rule"/>
	<section class="">
	    <div class="container" id="schedule">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class="titlesec"><span></span>Schedule</span>
			<br><br>
			<table class="table table-striped">
				<tbody>
				<tr>
					<th> Time (US Eastern Time, GMT-4)</th> <th> Time (Korean time, GMT+9)</th> 
				</tr>
				<tr>
					<td>	07:50 pm - 08:00 pm, July 9        </td><td>	08:50 am - 09:00 am , July 10       </td><td>  Organizers <br> <b> Introductory Remarks  </b> </td>
				</tr>
				<tr>
					<td>	08:00 pm - 08:35 pm, July 9        </td><td>	09:00 am - 09:35 am, July 10      </td><td>    Mark Ho <br> <b> Cognitive Science as a Source of Design Principles for Interactive Machine Learning </b> 
 					
 					<br>
 					<a data-toggle="collapse" data-target="#abstractmark" class="collapsed abstract" aria-expanded="false"> Abstract</a> 
								<div id="abstractmark" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									Computational cognitive science provides fundamental insights into how humans learn, think, decide, and interact, and provides a key bridge between the behavioral sciences and engineering disciplines. There are two broad ways in which cognitive science can inform the development of machines with human-like social intelligence. The first is to reverse-engineer human social intelligence itself---for example, by characterizing socio-cognitive processes such as theory of mind, communication, and cooperation. The second is to develop more accurate and principled theories of general cognition and decision-making---for instance, by incorporating notions of limited computational capacity into models of rational planning and action. I will discuss how these two approaches are complementary and how research on explaining and engineering social intelligence can be informed by their interaction.
								</div>
					</td>
				</tr>
					<tr>
					<td>	08:35 pm - 09:10 pm, July 9      </td><td>	09:35 am - 10:10 am, July 10      </td><td>   Scott Niekum <br> <b> Models of Human Preference for AI Alignment
 </b>
						<br>
						<a data-toggle="collapse" data-target="#abstractscott" class="collapsed abstract" aria-expanded="false"> Abstract</a> 
								<div id="abstractscott" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									Human preference elicitation is now at the core of some of the most successful contemporary approaches to reinforcement learning, imitation learning, and AI alignment in applications ranging from robotics to language modeling. However, such approaches typically rely on highly questionable assumptions about the meaning of human preferences — for example, that a preference between two trajectories implies that one has a higher ground-truth return (sum of rewards) than another. If the fundamental interpretation of preferences is flawed, then all of these promising approaches will also fall short, or even mislead practitioners into overestimating the alignment of AI agents. This talk will highlight some of these poor assumptions, as well as recent approaches for improving them, enabling a better understanding of human values and AI alignment with those values.
								</div>
 					</td>
				</tr>
				<tr>
					<td>	09:10 pm - 09:45 pm, July 9      </td><td>	
						10:10 am - 10:45 am, July 10   </td><td>       Yang Wu <br> <b> Emotion as information </b> 
						<br>
						<a data-toggle="collapse" data-target="#abstractyang" class="collapsed abstract" aria-expanded="false"> Abstract</a> 
								<div id="abstractyang" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									Adults’ interactions with young children are full of emotion: we smile, laugh, frown, and act surprised and delighted. How do children understand these expressions, and what roles do they play in human learning? Although emotional expressions are commonly considered indicators of how people feel, I propose that they are a powerful source of information that even very young children can use to reason about the world. Based on a computational framework, I will first show that children can harness emotional expressions to recover hidden aspects of the physical world, such as how a toy works, guiding early learning and exploration. Second, I will demonstrate that children can use emotional expressions to draw inferences about what others think and want, supporting a sophisticated understanding of the social world. Finally, building on the findings that humans think and learn by using emotion as information, I will discuss the implication of this research for artificial intelligence.
								</div>

 						</td>
				</tr>
				<tr>
					<td>	09:45 pm - 10:00 pm, July 9      </td><td>
						10:45 am - 11:00 am, July 10     </td><td>     Coffee Break

 </td>
				</tr>

				<tr>
					<td>	10:00 pm - 10:35 pm, July 9      </td><td>	
						11:00 am - 11:35 am, July 10   </td><td>       Henny Admoni <br> <b> Eye Gaze as Indicator of Mental States for Human-Robot Collaboration
 </b> 
						<br>
						<a data-toggle="collapse" data-target="#abstracthenny" class="collapsed abstract" aria-expanded="false"> Abstract</a> 
								<div id="abstracthenny" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									In robotics, human-robot collaboration works best when robots are responsive to their human partners' mental states. Human eye gaze has been used as a proxy for one such mental state: attention. While eye gaze can be a useful signal, for example enabling intent prediction, it is also a noisy one. Gaze serves several functions beyond attention, and thus recognizing what people are attending to from their eye gaze is a complex task. In this talk, I will discuss our research on modeling eye gaze to understand human attention in collaborative tasks such as shared manipulation and assisted driving.
								</div>

 						</td>
				</tr>
				
				
				<tr>
					<td>	10:35 pm, July 9 - 12:30 am, July 10      </td><td>
						11:35 am - 01:30 pm, July 10     </td><td>     Lunch Break                     

					</td>
				</tr>

				<tr><td>	
					12:30 am - 01:05 am, July 10     </td>
					<td>	
					01:30 pm - 02:05 pm, July 10     </td><td>     
						Yukie Nagai <br> <b> Predictive Coding Theory for Social Intelligence </b>
						<br>
						<a data-toggle="collapse" data-target="#abstractyukie" class="collapsed abstract" aria-expanded="false"> Abstract</a> 
								<div id="abstractyukie" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
								Predictive coding is a neuroscience theory that suggests the human brain functions as a predictive machine. The brain continually generates predictions about the world and minimizes prediction errors by updating internal models and taking actions in the environment. My research group has been exploring the potential of neural network models based on predictive coding. Our primary objective is to investigate the extent to which these models enable robots to acquire social intelligence.

								In this presentation, we will showcase our robot experiments, which demonstrate the successful development of social cognitive functions such as imitation, intention and emotion recognition,
and altruistic behavior. Neural networks with multimodal predictive processing have enabled
robots to acquire internal models for goal-directed actions and emotion expression, which can be
further used for interpreting the internal states of others. Furthermore, our experiments have
unveiled that modified predictive processing can lead to individual diversity, mirroring
observations in humans. We will discuss how our neuro-inspired approach contributes to
understanding human cognitive development and how it enhances interactions between humans
and robots.
								</div>
					</td>
				</tr>

				<tr>
					<td>	01:05 am - 02:00 am, July 10       </td>
						<td>	02:05 pm - 03:00 pm, July 10       </td><td>   <b> Contributed Talks</b>  <br>
					
						<ul>
							<li>
								Learning with Language-Guided State Abstractions
							</li>
							<li>
								Emotional Theory of Mind: Assessing Vision and Language Models' Capabilities and Limitations
							</li>
							<li>
								Concept Alignment as a Prerequisite for Value Alignment
							</li>
							<li>
								The Neuro-Symbolic Inverse Planning Engine
							</li>
							<li>
								Multimodal Interactive Fusion for Action Anticipation
							</li>
							<li>
								Singing the Body Electric: The Impact of Robot Embodiment on User Expectations
							</li>
							<li>
								Neuro-Symbolic Models of Human Moral Judgment: LLMs as Automatic Feature Extractors
							</li>
							<li>
								Personalized Re-Engagement Feedback for Adaptive Socially Assistive Robot Interventions
							</li>
							<li>
								Inferring the Future by Imagining the Past
							</li>
						</ul>

 					</td>
				</tr>

				<tr>
					<td>	02:00 am - 02:05 am, July 10       </td>
						<td>	
						03:00 pm - 03:05 pm, July 10    </td><td>   Organizers <br>  <b>    Concluding Remarks </b>
					</td>
				</tr>
			
				<tr><td>
						02:05 am - 02:30 am, July 10     </td>
					<td>
						03:05 pm - 03:30 pm, July 10     </td><td>     Coffee Break & Poster Session                  

					</td>
				</tr>

			    
				
				<tr><td>	 
						02:30 am - 03:00 am, July 10       </td>
					<td>	 
						03:30 pm - 04:00 pm, July 10       </td><td>  <b>  Poster Session </b>
					</td>
				</tr>
				

				</tbody>

			</table>

		    </div>
		</div>
	    </div>
	</section>


<!--  	<br>
	<hr class="half-rule"/>
<section id="callpapers">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Call for papers</span><br>
			   
			    
						   
			<br><br>
			<h5 style="font-weight: bold"> Areas of interest </h5>
			We will accept submissions focusing on Human and Robot Social Intelligence. Topics include but are not limited to:
			<br><br>
			<ol>
				<li style="padding-bottom: 10px"> <b> Social Perception and Theory of Mind:</b> How do humans acquire Theory of Mind capabilities? What computational models approximate these capabilities in humans?
				 </li>

				<li style="padding-bottom: 10px"> <b> Human Cooperation Competition and Communication</b>: Origins and development of human communication or competition; Inference of these relationships.
			  </li>
				<li style="padding-bottom: 10px"> <b> Machine Theory of Mind:</b> How to infer what an agent or robot knows about an environment? How to make robots learn about the minds of other robots or humans?
			 </li>
				<li style="padding-bottom: 10px"> <b> Human-Robot Interaction: </b> Methods to understand human intentions or offer assistance; social perception, communication between humans and robots; learning to convey information for better collaboration; probing and gathering information for collaboration. </li>
				<li style="padding-bottom: 10px"> <b> Interdisciplinary Studies: </b> We welcome works that integrate efforts in cognitive sciences and robotics to advance studies in both fields.
				</li>
			</ol>
					We welcome research papers of no more than 4 pages, not including references.
			<br><br>
						   The paper submission deadline is on <b>June 12th, 11:59 pm (AoE)</b>. Papers should be submitted to <a href="https://cmt3.research.microsoft.com/SIHR2023">https://cmt3.research.microsoft.com/SIHR2023</a>. Submissions should follow the <a href="https://roboticsconference.org/information/authorinfo/">RSS template</a>, and be submitted as .pdf files. The review process will be double blind, and therefore the papers should be appropriately anonymized.
			<br><br>
						
All accepted papers will be given oral presentations or poster presentations. For the oral presentations, authors would have the option to present in-person or remotely. The poster session will be held in-person and virtually on Gather.Town. Accepted papers will be made available online on the workshop website as non-archival reports, allowing submissions to future conferences or journals.    
						    <br><br>
			<h5 style="font-weight: bold"> Important Dates </h5>
			<ul>
		
			<li style="display: list-item">
				<b>Submission open:</b> Friday, May 12th, 2023 (11:59 pm, AoE).
			</li>

			<li style="display: list-item">
				<b>Submission deadline:</b> Monday, June 12th, 2023 (11:59 pm, AoE).
			</li>
			<li style="display: list-item">
				<b>Author Notifications:</b>  Friday, June 23rd, 2023.
				 
			</li>
				<li style="display: list-item">
					<b>Camera Ready:</b> Friday, June 30th, 2022 (11:59 pm, AoE).
				 
			</li>
					<li style="display: list-item">
					<b>Workshop:</b> Friday, July 10th (8:50 am - 4:00 pm Korean time, GMT+9), 2023. 
				 
			</li>
			</ul>
	    </div>
		</div>
			</div>
		    </div>
		</div>
	</section> 



	<hr class="half-rule"/>
	<section id="faq">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>Frequently Asked Questions</span><br>
			For more questions about the workshop, contact us at <a href="mailto:socialintelligenceworkshop@gmail.com">socialintelligenceworkshop@gmail.com</a>.
			<br>
			<br>
					<h5 style="font-weight: bold"> Will the workshop be online?</h5>
					<span> All the workshop be online. Invited talks and spotlights will be recorded, with live Q&A sessions and discussions.</span>
					<br>
					<br>
					<h5 style="font-weight: bold"> How can I attend this workshop?</h5>
					<span> We will be publishing here the Zoom and Gather.Town links to attend the workshop. Stay tuned!   </span>
					<br>
					<br>
					<h5 style="font-weight: bold"> Can I submit previously published work?</h5>
					<span> We welcome new unpublished work into the workshop. You can however submit work that is concurrently under review in a conference or joural. </span>
					<br>
					<br>
					<h5 style="font-weight: bold"> What happens if my paper is accepted at the workshop? </h5>
					<span> We will post your paper in the workshop website, and you will be abe to present a poster through <a href="https://gather.town/">Gather.town</a>. Some selected papers will also have an oral presentation, which will be recorded and played during the workshop. Note that accepted papers will be non-archival and they can still be submitted in other journals and conferences.</span>

		    </div>
		</div>
	    </div>
	</section>  -->

	<hr class="half-rule"/>
	<section id="papers">

		
		
		<div class="container">
			<div class="row">
			    <div class="col-md-10 mx-auto">
					<span class=titlesec>Papers</span><br>
					<!-- Thank you everyone for submitting your papers to this workshop! All the papers below will be presented in the Poster Session (link TBA). Spotlight papers will additio nally be presented as short talks during the Spotlight sessions. -->
					<br><br>
					<!-- <h5 style="font-weight: bold"> Spotlights </h5> -->
					<!-- Congratulations to the Spotlight Papers!  -->
					<ul class="listpapers">
	
						<li> 
							 <span class="postername">Poster #1 - <span>
							Learning with Language-Guided State Abstractions 
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Andi Peng (MIT); Ilia Sucholutsky (Princeton University); Belinda Li (MIT); Theodore Sumers (Princeton); Tom Griffiths (UC Berkeley); Jacob Andreas (MIT); Julie A. Shah
							</span>
						</li>

						<li> 
							 <span class="postername">Poster #2 - <span>	
							Emotional Theory of Mind: Assessing Vision and Language Models' Capabilities and Limitations 
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Yasaman Etesam (Simon Fraser University); Ozge Nilay Yalcin (Simon Fraser University); Chuxuan Zhang (Simon Fraser University); Angelica Lim (Simon Fraser University)
							</span>
						</li>

						<li> 
							 <span class="postername">Poster #3 - <span>	
							Concept Alignment as a Prerequisite for Value Alignment 
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Sunayana Rane (Princeton University); Mark K Ho (Princeton University); Ilia Sucholutsky (Princeton University); Tom Griffiths (UC Berkeley)
							</span>
						</li>


						<li> 
							 <span class="postername">Poster #4 - <span>	
							The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs 
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Lance Ying (Harvard University); Katherine M Collins (University of Cambridge); Megan Wei (Massachusetts Institute of Technology); Cedegao Zhang (Massachusetts Institute of Technology); Tan Zhi-Xuan (Massachusetts Institute of Technology); Adrian Weller (University of Cambridge); Joshua Tenenbaum (MIT); Catherine Wong (Massachusetts Institute of Technology)
							</span>
						</li>

						<li> 
							 <span class="postername">Poster #5 - <span>	
							Multimodal Interactive Fusion for Action Anticipation
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Zongnan Ma (Yanan University); Zhixiong Nan (Chongqing University); Fuchun Zhang (Yanan University)
							</span>
						</li>


						<li> 
							 <span class="postername">Poster #6 - <span>	
							Singing the Body Electric: The Impact of Robot Embodiment on User Expectations
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Nathaniel S Dennler (University of Southern California); Stefanos Nikolaidis (USC); Maja Mataric ((University of Southern California, US))
							</span>
						</li>


						<li> 
							 <span class="postername">Poster #7 - <span>	
							Neuro-Symbolic Models of Human Moral Judgment: LLMs as Automatic Feature Extractors
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Joseph Kwon (MIT); Sydney Levine (Allen Institute for AI); Joshua Tenenbaum (MIT)
							</span>
						</li>


						<li> 
							 <span class="postername">Poster #8 - <span>	
							Personalized Re-Engagement Feedback for Adaptive Socially Assistive Robot Interventions
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Zhonghao Shi (University of Southern California); Allison O’Connell (University of Southern California); Maja Mataric ((University of Southern California, US))
							</span>
						</li>


						<li> 
							 <span class="postername">Poster #9 - <span>	
							Inferring the Future by Imagining the Past
								 <a class="linkpaper" href=""> [link]</a> <br> 
							<span class="authorname">
							Kartik Chandra (MIT); Tony Chen (MIT); Tzu-Mao Li (University of California, San Diego); Jonathan Ragan-Kelley (MIT); Joshua Tenenbaum (MIT)
							</span>
						</li>


					</ul>

				





					<br><br>



					<h5 style="font-weight: bold"> Reviewers </h5>
					We thank the following people for their assistance in reviewing submitted papers.
					<br><br>
					<div class="row">
						<div class="col-md-4">
						<ul>
							<li> Aishni Parab
							</li><li>Andi Peng
							</li><li> Aviv Netanyahu
							</li><li> Emma Hughson
							</li><li> Joe Kwon
							</li><li> Kaiwen Jiang
							
						</ul>
						</div>
						<div class="col-md-4">
						<ul>

							</li><li> Kartik Chandra
							</li><li> Lance Ying
							</li><li> Lifeng Fan
							</li><li> Marta Kryven
							</li><li> Micol Spitale
							</li><li> Minglu Zhao

							</li>
						</ul>
						</div>
						<div class="col-md-4">
						<ul>
							<!-- </li><li> Priyam Parashar -->
							</li><li> Sooyeon Jeong
							</li><li> Tan Zhi-Xuan
							</li><li> Xiaofeng Gao
							</li><li> Yen-Ling Kuo
							</li><li> Zhitian Zhang	
							</li>
						</ul>
						</div>
					</div>

				</div>
			</div>
		</div>
	</section>

	<hr class="half-rule"/>
	<section id="organizers">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">

		<span class=titlesec>Organizers</span><br>
		<div class="row">
			
			<a href='https://www.tshu.io/'>
		    <div class="profpic xlarge-1 columns">

			<img  src=../images/people/tshu.png class="figure-img img-fluid ">
			<p class=profname>  <a href="https://www.tshu.io/"> Tianmin Shu </a> </p>
			<p class=institution>Massachusetts Institute of Technology</p>
		    </div>
			</a>
			
			<a href='http://people.csail.mit.edu/xavierpuig/'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/xavi.jpg class="figure-img img-fluid ">
			<p class=profname><a href="http://people.csail.mit.edu/xavierpuig">Xavier Puig</a></p>
			<p class=institution>Meta AI</p>
		    </div>
			</a>


			<a href='http://www.mit.edu/~lishuang/'>
		    <div class="profpic xlarge-1 columns">
			    <img src=../images/people/lishuang.jpg class="figure-img img-fluid ">
			<p class=profname><a href="http://www.mit.edu/~lishuang/"> Shuang Li </a> </p>
			<p class=institution> Massachusetts Institute of Technology </p>
		    </div>
			</a>

			<a href='https://www.alihkw.com/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src="../images/people/ali.jpeg" class="figure-img img-fluid">
			<p class=profname><a href="https://www.alihkw.com/"> Ali Kuwajerwala </a></p>
			<p class=institution> UdeM & Mila</p>
		    </div>
			</a>

			</div>
			<div class="row">

			<a href='https://pratyushasharma.github.io/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src="../images/people/pratyusha.jpeg" class="figure-img img-fluid">
			<p class=profname><a href="https://pratyushasharma.github.io/"> Pratyusha Sharma </a></p>
			<p class=institution> Massachusetts Institute of Technology</p>
		    </div>
			</a>



		
			<a href='https://chocobearz.github.io/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src="../images/people/paige.jpeg" class="figure-img img-fluid">
			<p class=profname><a href="https://chocobearz.github.io/"> Paige Tuttösí  </a></p>
			<p class=institution> Simon Fraser University </p>
		    </div>
			</a>
			<a href='https://angelicalim.com/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src='https://media.licdn.com/dms/image/C5603AQE8mGSpTSsW_g/profile-displayphoto-shrink_800_800/0/1661567065131?e=2147483647&v=beta&t=mS7lIMhZ6NpDyMSSLfXUzo2E2hxq-iHv0UsjwELlRIM' class="figure-img img-fluid ">
			<p class="profname"> <a href="https://angelicalim.com/"> Angelica Lim </a></p>
			<p class=institution>Simon Fraser University</p>
		    </div>
			</a>

		   <a href='http://web.mit.edu/torralba/www/'>
		    <div class="profpic xlarge-1 columns">
			    <img  src=../images/people/torralba.jpg class="figure-img img-fluid ">
			<p class=profname><a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a></p>
			<p class=institution> Massachusetts Institute of Technology</p>
		    </div>
		</a>


		</div>


		</div>
</div>

	</section>



	<br>
	<hr class="half-rule"/>
	<section id="contact">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Contact</span><br>
		Reach out to <a href="mailto:socialintelligenceworkshop@gmail.com">socialintelligenceworkshop@gmail.com</a> for any questions.
	    </div>
		</div>
		</div>
	</section>

	<!-- Footer -->


	<!-- Bootstrap core JavaScript -->
<!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	<!-- Plugin JavaScript -->
	<!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> --> -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js



"> </script>

	<!-- Custom JavaScript for this theme -->
	<script src="js/scrolling-nav.js"></script>

    </body>


</html>
